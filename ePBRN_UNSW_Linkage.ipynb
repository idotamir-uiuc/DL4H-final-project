{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce results of Scheme B\n",
    "\n",
    "Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
    "\n",
    "Kha Vo, Jitendra Jonnagaddala, Siaw-Teng Liaw\n",
    "\n",
    "February 2019\n",
    "\n",
    "Jounal of Biomedical Informatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = 'ePBRN_F_dup' \n",
    "testset = 'ePBRN_D_dup'\n",
    "\n",
    "import recordlinkage as rl, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from recordlinkage.preprocessing import phonetic\n",
    "from numpy.random import choice\n",
    "import collections, numpy\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "def generate_true_links(df): \n",
    "    # although the match_id column is included in the original df to imply the true links,\n",
    "    # this function will create the true_link object identical to the true_links properties\n",
    "    # of recordlinkage toolkit, in order to exploit \"Compare.compute()\" from that toolkit\n",
    "    # in extract_function() for extracting features quicker.\n",
    "    # This process should be deprecated in the future release of the UNSW toolkit.\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    processed = 0\n",
    "    for match_id in df[\"match_id\"].unique():\n",
    "        if match_id != -1:    \n",
    "            processed = processed + 1\n",
    "            # print(\"In routine generate_true_links(), count =\", processed)\n",
    "            # clear_output(wait=True)\n",
    "            linkages = df.loc[df['match_id'] == match_id]\n",
    "            for j in range(len(linkages)-1):\n",
    "                for k in range(j+1, len(linkages)):\n",
    "                    indices_1 = indices_1 + [linkages.iloc[j][\"rec_id\"]]\n",
    "                    indices_2 = indices_2 + [linkages.iloc[k][\"rec_id\"]]    \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def generate_false_links(df, size):\n",
    "    # A counterpart of generate_true_links(), with the purpose to generate random false pairs\n",
    "    # for training. The number of false pairs in specified as \"size\".\n",
    "    df[\"rec_id\"] = df.index.values.tolist()\n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    unique_match_id = df[\"match_id\"].unique()\n",
    "    unique_match_id = unique_match_id[~np.isnan(unique_match_id)] # remove nan values\n",
    "    for j in range(size):\n",
    "            false_pair_ids = choice(unique_match_id, 2)\n",
    "            candidate_1_cluster = df.loc[df['match_id'] == false_pair_ids[0]]\n",
    "            candidate_1 = candidate_1_cluster.iloc[choice(range(len(candidate_1_cluster)))]\n",
    "            candidate_2_cluster = df.loc[df['match_id'] == false_pair_ids[1]]\n",
    "            candidate_2 = candidate_2_cluster.iloc[choice(range(len(candidate_2_cluster)))]    \n",
    "            indices_1 = indices_1 + [candidate_1[\"rec_id\"]]\n",
    "            indices_2 = indices_2 + [candidate_2[\"rec_id\"]]  \n",
    "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
    "    return links\n",
    "\n",
    "def swap_fields_flag(f11, f12, f21, f22):\n",
    "    return ((f11 == f22) & (f12 == f21)).astype(float)\n",
    "\n",
    "def join_names_space(f11, f12, f21, f22):\n",
    "    return ((f11+\" \"+f12 == f21) | (f11+\" \"+f12 == f22)| (f21+\" \"+f22 == f11)| (f21+\" \"+f22 == f12)).astype(float)\n",
    "\n",
    "def join_names_dash(f11, f12, f21, f22):\n",
    "    return ((f11+\"-\"+f12 == f21) | (f11+\"-\"+f12 == f22)| (f21+\"-\"+f22 == f11)| (f21+\"-\"+f22 == f12)).astype(float)\n",
    "\n",
    "def abb_surname(f1, f2):\n",
    "    return ((f1[0]==f2) | (f1==f2[0])).astype(float)\n",
    "\n",
    "def reset_day(f11, f12, f21, f22):\n",
    "    return (((f11 == 1) & (f12 == 1))|((f21 == 1) & (f22 == 1))).astype(float)\n",
    "\n",
    "def extract_features(df, links):\n",
    "    c = rl.Compare()\n",
    "    c.string('given_name', 'given_name', method='levenshtein', label='y_name_leven')\n",
    "    c.string('surname', 'surname', method='levenshtein', label='y_surname_leven')  \n",
    "    c.string('given_name', 'given_name', method='jarowinkler', label='y_name_jaro')\n",
    "    c.string('surname', 'surname', method='jarowinkler', label='y_surname_jaro')  \n",
    "    c.string('postcode', 'postcode', method='jarowinkler', label='y_postcode')      \n",
    "    exact_fields = ['postcode', 'address_1', 'address_2', 'street_number']\n",
    "    for field in exact_fields:\n",
    "        c.exact(field, field, label='y_'+field+'_exact')\n",
    "    c.compare_vectorized(reset_day,('day', 'month'), ('day', 'month'),label='reset_day_flag')    \n",
    "    c.compare_vectorized(swap_fields_flag,('day', 'month'), ('day', 'month'),label='swap_day_month')    \n",
    "    c.compare_vectorized(swap_fields_flag,('surname', 'given_name'), ('surname', 'given_name'),label='swap_names')    \n",
    "    c.compare_vectorized(join_names_space,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_space')\n",
    "    c.compare_vectorized(join_names_dash,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_dash')\n",
    "    c.compare_vectorized(abb_surname,'surname', 'surname',label='abb_surname')\n",
    "    # Build features\n",
    "    feature_vectors = c.compute(links, df, df)\n",
    "    return feature_vectors\n",
    "\n",
    "def generate_train_X_y(df):\n",
    "    # This routine is to generate the feature vector X and the corresponding labels y\n",
    "    # with exactly equal number of samples for both classes to train the classifier.\n",
    "    pos = extract_features(df, train_true_links)\n",
    "    train_false_links = generate_false_links(df, len(train_true_links))    \n",
    "    neg = extract_features(df, train_false_links)\n",
    "    X = pos.values.tolist() + neg.values.tolist()\n",
    "    y = [1]*len(pos)+[0]*len(neg)\n",
    "    X, y = shuffle(X, y, random_state=0)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def train_model(modeltype, modelparam, train_vectors, train_labels, modeltype_2):\n",
    "    if modeltype == 'svm': # Support Vector Machine\n",
    "        model = svm.SVC(C = modelparam, kernel = modeltype_2)\n",
    "        model.fit(train_vectors, train_labels) \n",
    "    elif modeltype == 'lg': # Logistic Regression\n",
    "        model = LogisticRegression(C=modelparam, penalty = modeltype_2,class_weight=None, dual=False, fit_intercept=True, \n",
    "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
    "                                   n_jobs=1, random_state=None)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nb': # Naive Bayes\n",
    "        model = GaussianNB()\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    elif modeltype == 'nn': # Neural Network\n",
    "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
    "                              activation = modeltype_2,random_state=None, batch_size='auto', \n",
    "                              learning_rate='constant',  learning_rate_init=0.001, \n",
    "                              power_t=0.5, max_iter=30000, shuffle=True, \n",
    "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
    "                              nesterovs_momentum=True, early_stopping=False, \n",
    "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        model.fit(train_vectors, train_labels)\n",
    "    return model\n",
    "\n",
    "def classify(model, test_vectors):\n",
    "    result = model.predict(test_vectors)\n",
    "    return result\n",
    "\n",
    "    \n",
    "def evaluation(test_labels, result):\n",
    "    true_pos = np.logical_and(test_labels, result)\n",
    "    count_true_pos = np.sum(true_pos)\n",
    "    true_neg = np.logical_and(np.logical_not(test_labels),np.logical_not(result))\n",
    "    count_true_neg = np.sum(true_neg)\n",
    "    false_pos = np.logical_and(np.logical_not(test_labels), result)\n",
    "    count_false_pos = np.sum(false_pos)\n",
    "    false_neg = np.logical_and(test_labels,np.logical_not(result))\n",
    "    count_false_neg = np.sum(false_neg)\n",
    "    precision = count_true_pos/(count_true_pos+count_false_pos)\n",
    "    sensitivity = count_true_pos/(count_true_pos+count_false_neg) # sensitivity = recall\n",
    "    confusion_matrix = [count_true_pos, count_false_pos, count_false_neg, count_true_neg]\n",
    "    no_links_found = np.count_nonzero(result)\n",
    "    no_false = count_false_pos + count_false_neg\n",
    "    Fscore = 2*precision*sensitivity/(precision+sensitivity)\n",
    "    metrics_result = {'no_false':no_false, 'confusion_matrix':confusion_matrix ,'precision':precision,\n",
    "                     'sensitivity':sensitivity ,'no_links':no_links_found, 'F-score': Fscore}\n",
    "    return metrics_result\n",
    "\n",
    "def blocking_performance(candidates, true_links, df):\n",
    "    count = 0\n",
    "    for candi in candidates:\n",
    "        if df.loc[candi[0]][\"match_id\"]==df.loc[candi[1]][\"match_id\"]:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import train set...\n",
      "Train set size: 14078 , number of matched pairs:  3192\n",
      "Finished building X_train, y_train\n"
     ]
    }
   ],
   "source": [
    "## TRAIN SET CONSTRUCTION\n",
    "\n",
    "# Import\n",
    "print(\"Import train set...\")\n",
    "df_train = pd.read_csv(trainset+\".csv\", index_col = \"rec_id\")\n",
    "train_true_links = generate_true_links(df_train)\n",
    "print(\"Train set size:\", len(df_train), \", number of matched pairs: \", str(len(train_true_links)))\n",
    "\n",
    "# Preprocess train set\n",
    "df_train['postcode'] = df_train['postcode'].astype(str)\n",
    "\n",
    "# Final train feature vectors and labels\n",
    "X_train, y_train = generate_train_X_y(df_train)\n",
    "print(\"Finished building X_train, y_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6384, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import test set...\n",
      "Test set size: 11731 , number of matched pairs:  2653\n",
      "BLOCKING PERFORMANCE:\n",
      "Number of pairs of matched given_name: 252552 , detected  1567 /2653 true matched pairs, missed 1086\n",
      "Number of pairs of matched surname: 33832 , detected  1480 /2653 true matched pairs, missed 1173\n",
      "Number of pairs of matched postcode: 79940 , detected  2462 /2653 true matched pairs, missed 191\n",
      "Number of pairs of at least 1 field matched: 362910 , detected  2599 /2653 true matched pairs, missed 54\n"
     ]
    }
   ],
   "source": [
    "# Blocking Criteria: declare non-match of all of the below fields disagree\n",
    "# Import\n",
    "print(\"Import test set...\")\n",
    "df_test = pd.read_csv(testset+\".csv\", index_col = \"rec_id\")\n",
    "test_true_links = generate_true_links(df_test)\n",
    "leng_test_true_links = len(test_true_links)\n",
    "print(\"Test set size:\", len(df_test), \", number of matched pairs: \", str(leng_test_true_links))\n",
    "\n",
    "print(\"BLOCKING PERFORMANCE:\")\n",
    "blocking_fields = [\"given_name\", \"surname\", \"postcode\"]\n",
    "all_candidate_pairs = []\n",
    "for field in blocking_fields:\n",
    "    block_indexer = rl.Index()\n",
    "    block_indexer.block(on=field)\n",
    "#     block_indexer = rl.BlockIndex(on=field)\n",
    "    candidates = block_indexer.index(df_test)\n",
    "    detects = blocking_performance(candidates, test_true_links, df_test)\n",
    "    all_candidate_pairs = candidates.union(all_candidate_pairs)\n",
    "    print(\"Number of pairs of matched \"+ field +\": \"+str(len(candidates)), \", detected \",\n",
    "         detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )\n",
    "detects = blocking_performance(all_candidate_pairs, test_true_links, df_test)\n",
    "print(\"Number of pairs of at least 1 field matched: \" + str(len(all_candidate_pairs)), \", detected \",\n",
    "     detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
    "          str(leng_test_true_links-detects) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set...\n",
      "Preprocess...\n",
      "Extract feature vectors...\n",
      "Count labels of y_test: Counter({0: 360311, 1: 2599})\n",
      "Finished building X_test, y_test\n"
     ]
    }
   ],
   "source": [
    "## TEST SET CONSTRUCTION\n",
    "\n",
    "# Preprocess test set\n",
    "print(\"Processing test set...\")\n",
    "print(\"Preprocess...\")\n",
    "df_test['postcode'] = df_test['postcode'].astype(str)\n",
    "\n",
    "# Test feature vectors and labels construction\n",
    "print(\"Extract feature vectors...\")\n",
    "df_X_test = extract_features(df_test, all_candidate_pairs)\n",
    "vectors = df_X_test.values.tolist()\n",
    "labels = [0]*len(vectors)\n",
    "feature_index = df_X_test.index\n",
    "for i in range(0, len(feature_index)):\n",
    "    if df_test.loc[feature_index[i][0]][\"match_id\"]==df_test.loc[feature_index[i][1]][\"match_id\"]:\n",
    "        labels[i] = 1\n",
    "X_test, y_test = shuffle(vectors, labels, random_state=0)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"Count labels of y_test:\",collections.Counter(y_test))\n",
    "print(\"Finished building X_test, y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(a):\n",
    "    np_a = np.array(a)\n",
    "    i = np.argmax(np_a)\n",
    "    return np_a[i], i\n",
    "\n",
    "def argmin(a):\n",
    "    np_a = np.array(a)\n",
    "    i = np.argmin(np_a)\n",
    "    return np_a[i], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
      "Model: svm , Param_1: linear , tuning range: [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n",
      "No_false: [1214, 1776, 49907, 77590, 77634, 77636, 77636, 78097, 78273, 79729, 85610, 86036, 92358, 88894, 92399, 92398, 92397, 91637, 91618, 91522] \n",
      "\n",
      "Precision: [0.6916689731525049, 0.5976275207591933, 0.049052503145613296, 0.032200915748630744, 0.03214824606252572, 0.03214744429064881, 0.032217442802817776, 0.0321841074703812, 0.03211406242271356, 0.0315574666569492, 0.029464113639197813, 0.02932250239747278, 0.027370283391429804, 0.028406544762987332, 0.027358470704646413, 0.027358758697643083, 0.02735904669670302, 0.02757969353537926, 0.027585255314858254, 0.0276133912729359] \n",
      "\n",
      "Sensitivity: [0.9615236629472874, 0.9692189303578299, 0.9899961523662947, 0.9930742593305117, 0.9919199692189303, 0.9919199692189303, 0.9942285494420932, 0.9992304732589458, 0.9992304732589458, 0.9996152366294728, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] \n",
      "\n",
      "F-score: [0.804571796522859, 0.7393601408864102, 0.09347356184040832, 0.0623791569992266, 0.06227805290494021, 0.06227654845878829, 0.06241244384329258, 0.06235967871678813, 0.06222818598967257, 0.06118339711510157, 0.057241652717822215, 0.05697437358879365, 0.05328221739308705, 0.055243803936572716, 0.053259833806367, 0.05326037952375097, 0.05326092525231825, 0.05367893840037176, 0.05368947281441084, 0.05374276261373035] \n",
      "\n",
      "MIN No_false: (1214, 0) \n",
      "\n",
      "MAX Precision: (0.6916689731525049, 0) \n",
      "\n",
      "MAX Sensitivity: (1.0, 10) \n",
      "\n",
      "MAX F-score: (0.804571796522859, 0) \n",
      "\n",
      "\n",
      "Model: svm , Param_1: rbf , tuning range: [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n",
      "No_false: [5401, 9039, 75671, 81575, 81410, 78628, 78422, 78841, 92218, 104857, 112217, 111471, 108474, 103011, 101578, 91553, 97656, 104580, 96773, 90030] \n",
      "\n",
      "Precision: [0.32319535588086823, 0.22227013972744522, 0.033181713643216725, 0.030865370907190038, 0.030925994262382895, 0.03198522622345337, 0.03206655229020353, 0.03190156929197672, 0.02741069639410654, 0.024186643835616438, 0.022636217948717948, 0.02278425528184448, 0.023390443950266047, 0.024600409059919705, 0.02494792516582355, 0.02759426447158789, 0.02592389407012119, 0.02424915328562498, 0.02615424868172121, 0.02804797737160871] \n",
      "\n",
      "Sensitivity: [0.9853789919199692, 0.9915352058484033, 0.9992304732589458, 0.9996152366294728, 0.9996152366294728, 0.9996152366294728, 0.9996152366294728, 0.9996152366294728, 1.0, 1.0, 1.0, 1.0, 0.9996152366294728, 0.9996152366294728, 1.0, 0.9996152366294728, 1.0, 1.0, 1.0, 0.9996152366294728] \n",
      "\n",
      "F-score: [0.48674332414710625, 0.36313675755654196, 0.06423050763618376, 0.059881757730117204, 0.05999584324411703, 0.06198702042374498, 0.06213973067999713, 0.061829908254697336, 0.05335879116366921, 0.04723092998955068, 0.04427032321253673, 0.04455339464639279, 0.045711269464238584, 0.04801907455155397, 0.04868135161459504, 0.05370598145717268, 0.0505376553172458, 0.047350106578731614, 0.0509752772847182, 0.05456492974607775] \n",
      "\n",
      "MIN No_false: (5401, 0) \n",
      "\n",
      "MAX Precision: (0.32319535588086823, 0) \n",
      "\n",
      "MAX Sensitivity: (1.0, 8) \n",
      "\n",
      "MAX F-score: (0.48674332414710625, 0) \n",
      "\n",
      "\n",
      "Model: lg , Param_1: l1 , tuning range: [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel:\u001B[39m\u001B[38;5;124m\"\u001B[39m, model,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, Param_1:\u001B[39m\u001B[38;5;124m\"\u001B[39m,model_type, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, tuning range:\u001B[39m\u001B[38;5;124m\"\u001B[39m, modelparam_range)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m modelparam \u001B[38;5;129;01min\u001B[39;00m modelparam_range:\n\u001B[0;32m---> 19\u001B[0m     md \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodelparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     final_result \u001B[38;5;241m=\u001B[39m classify(md, X_test)\n\u001B[1;32m     21\u001B[0m     final_eval \u001B[38;5;241m=\u001B[39m evaluation(y_test, final_result)\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(modeltype, modelparam, train_vectors, train_labels, modeltype_2)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m modeltype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlg\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;66;03m# Logistic Regression\u001B[39;00m\n\u001B[1;32m    113\u001B[0m     model \u001B[38;5;241m=\u001B[39m LogisticRegression(C\u001B[38;5;241m=\u001B[39mmodelparam, penalty \u001B[38;5;241m=\u001B[39m modeltype_2,class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dual\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, fit_intercept\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \n\u001B[1;32m    114\u001B[0m                                intercept_scaling\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5000\u001B[39m, multi_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124movr\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[1;32m    115\u001B[0m                                n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 116\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_vectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m modeltype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnb\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;66;03m# Naive Bayes\u001B[39;00m\n\u001B[1;32m    118\u001B[0m     model \u001B[38;5;241m=\u001B[39m GaussianNB()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1091\u001B[0m, in \u001B[0;36mLogisticRegression.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   1062\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1063\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1064\u001B[0m \u001B[38;5;124;03m    Fit the model according to the given training data.\u001B[39;00m\n\u001B[1;32m   1065\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1089\u001B[0m \u001B[38;5;124;03m    The SAGA solver supports both float64 and float32 bit arrays.\u001B[39;00m\n\u001B[1;32m   1090\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1091\u001B[0m     solver \u001B[38;5;241m=\u001B[39m \u001B[43m_check_solver\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msolver\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpenalty\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdual\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1093\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC, numbers\u001B[38;5;241m.\u001B[39mNumber) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1094\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPenalty term must be positive; got (C=\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:61\u001B[0m, in \u001B[0;36m_check_solver\u001B[0;34m(solver, penalty, dual)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     56\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogistic Regression supports only penalties in \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     57\u001B[0m         \u001B[38;5;241m%\u001B[39m (all_penalties, penalty)\n\u001B[1;32m     58\u001B[0m     )\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m solver \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mliblinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msaga\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m penalty \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ml2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m---> 61\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     62\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSolver \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m supports only \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml2\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m penalties, got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m penalty.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;241m%\u001B[39m (solver, penalty)\n\u001B[1;32m     64\u001B[0m     )\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m solver \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mliblinear\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m dual:\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     67\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSolver \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m supports only dual=False, got dual=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (solver, dual)\n\u001B[1;32m     68\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty."
     ]
    }
   ],
   "source": [
    "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
    "# Choose model\n",
    "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
    "models = {\n",
    "    'svm': ['linear', 'rbf'],\n",
    "    'lg': [None, 'l2'],\n",
    "    'nn': ['relu', 'logistic'],\n",
    "}\n",
    "\n",
    "modelparam_range = [.001,.002,.005,.01,.02,.05,.1,.2,.5,1,5,10,20,50,100,200,500,1000,2000,5000] # C for svm, C for lg, alpha for NN\n",
    "for model, model_types in models.items():\n",
    "    for model_type in model_types:\n",
    "        precision = []\n",
    "        sensitivity = []\n",
    "        Fscore = []\n",
    "        nb_false = []\n",
    "        print(\"Model:\", model,\", Param_1:\",model_type, \", tuning range:\", modelparam_range)\n",
    "        for modelparam in modelparam_range:\n",
    "            md = train_model(model, modelparam, X_train, y_train, model_type)\n",
    "            final_result = classify(md, X_test)\n",
    "            final_eval = evaluation(y_test, final_result)\n",
    "            precision += [final_eval['precision']]\n",
    "            sensitivity += [final_eval['sensitivity']]\n",
    "            Fscore += [final_eval['F-score']]\n",
    "            nb_false  += [final_eval['no_false']]\n",
    "        print(\"No_false:\",nb_false,\"\\n\")\n",
    "        print(\"Precision:\",precision,\"\\n\")\n",
    "        print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
    "        print(\"F-score:\", Fscore,\"\\n\")\n",
    "\n",
    "        print(\"MIN No_false:\",argmin(nb_false),\"\\n\")\n",
    "        print(\"MAX Precision:\",argmax(precision),\"\\n\")\n",
    "        print(\"MAX Sensitivity:\",argmax(sensitivity),\"\\n\")\n",
    "        print(\"MAX F-score:\", argmax(Fscore),\"\\n\")\n",
    "        print(\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAGGING PERFORMANCE:\n",
      "\n",
      "svm per fold:\n",
      "Fold 0 {'no_false': 4346, 'confusion_matrix': [2544, 4291, 55, 356020], 'precision': 0.37220190197512804, 'sensitivity': 0.9788380146210081, 'no_links': 6835, 'F-score': 0.5393258426966293}\n",
      "Fold 1 {'no_false': 4140, 'confusion_matrix': [2544, 4085, 55, 356226], 'precision': 0.38376829084326447, 'sensitivity': 0.9788380146210081, 'no_links': 6629, 'F-score': 0.5513654096228869}\n",
      "Fold 2 {'no_false': 4151, 'confusion_matrix': [2544, 4096, 55, 356215], 'precision': 0.38313253012048193, 'sensitivity': 0.9788380146210081, 'no_links': 6640, 'F-score': 0.5507089511851931}\n",
      "Fold 3 {'no_false': 4360, 'confusion_matrix': [2546, 4307, 53, 356004], 'precision': 0.3715161243251131, 'sensitivity': 0.9796075413620623, 'no_links': 6853, 'F-score': 0.5387219636055861}\n",
      "Fold 4 {'no_false': 4302, 'confusion_matrix': [2541, 4244, 58, 356067], 'precision': 0.37450257921886515, 'sensitivity': 0.9776837245094268, 'no_links': 6785, 'F-score': 0.5415601023017903}\n",
      "Fold 5 {'no_false': 4219, 'confusion_matrix': [2544, 4164, 55, 356147], 'precision': 0.37924865831842575, 'sensitivity': 0.9788380146210081, 'no_links': 6708, 'F-score': 0.5466852906414527}\n",
      "Fold 6 {'no_false': 4341, 'confusion_matrix': [2546, 4288, 53, 356023], 'precision': 0.37254901960784315, 'sensitivity': 0.9796075413620623, 'no_links': 6834, 'F-score': 0.5398070603201526}\n",
      "Fold 7 {'no_false': 4174, 'confusion_matrix': [2542, 4117, 57, 356194], 'precision': 0.3817389998498273, 'sensitivity': 0.9780684878799538, 'no_links': 6659, 'F-score': 0.5491466839490171}\n",
      "Fold 8 {'no_false': 4380, 'confusion_matrix': [2544, 4325, 55, 355986], 'precision': 0.3703595865482603, 'sensitivity': 0.9788380146210081, 'no_links': 6869, 'F-score': 0.5373891001267427}\n",
      "Fold 9 {'no_false': 4155, 'confusion_matrix': [2544, 4100, 55, 356211], 'precision': 0.38290186634557494, 'sensitivity': 0.9788380146210081, 'no_links': 6644, 'F-score': 0.5504706264199934}\n",
      "svm bagging: {'no_false': 4231, 'confusion_matrix': [2544, 4176, 55, 356135], 'precision': 0.37857142857142856, 'sensitivity': 0.9788380146210081, 'no_links': 6720, 'F-score': 0.5459813284687198}\n",
      "\n",
      "nn per fold:\n",
      "Fold 0 {'no_false': 1186, 'confusion_matrix': [2507, 1094, 92, 359217], 'precision': 0.6961955012496529, 'sensitivity': 0.9646017699115044, 'no_links': 3601, 'F-score': 0.8087096774193548}\n",
      "Fold 1 {'no_false': 1043, 'confusion_matrix': [2506, 950, 93, 359361], 'precision': 0.7251157407407407, 'sensitivity': 0.9642170065409773, 'no_links': 3456, 'F-score': 0.8277456647398843}\n",
      "Fold 2 {'no_false': 1218, 'confusion_matrix': [2507, 1126, 92, 359185], 'precision': 0.6900633085604184, 'sensitivity': 0.9646017699115044, 'no_links': 3633, 'F-score': 0.8045571245186137}\n",
      "Fold 3 {'no_false': 1024, 'confusion_matrix': [2506, 931, 93, 359380], 'precision': 0.7291242362525459, 'sensitivity': 0.9642170065409773, 'no_links': 3437, 'F-score': 0.8303512259774685}\n",
      "Fold 4 {'no_false': 1095, 'confusion_matrix': [2506, 1002, 93, 359309], 'precision': 0.7143671607753705, 'sensitivity': 0.9642170065409773, 'no_links': 3508, 'F-score': 0.8206975601768463}\n",
      "Fold 5 {'no_false': 1014, 'confusion_matrix': [2506, 921, 93, 359390], 'precision': 0.7312518237525533, 'sensitivity': 0.9642170065409773, 'no_links': 3427, 'F-score': 0.8317291735811484}\n",
      "Fold 6 {'no_false': 1106, 'confusion_matrix': [2506, 1013, 93, 359298], 'precision': 0.7121341290139244, 'sensitivity': 0.9642170065409773, 'no_links': 3519, 'F-score': 0.8192219679633868}\n",
      "Fold 7 {'no_false': 1265, 'confusion_matrix': [2510, 1176, 89, 359135], 'precision': 0.6809549647314161, 'sensitivity': 0.9657560600230858, 'no_links': 3686, 'F-score': 0.7987271280827367}\n",
      "Fold 8 {'no_false': 1154, 'confusion_matrix': [2506, 1061, 93, 359250], 'precision': 0.7025511634426689, 'sensitivity': 0.9642170065409773, 'no_links': 3567, 'F-score': 0.8128446318520921}\n",
      "Fold 9 {'no_false': 1135, 'confusion_matrix': [2506, 1042, 93, 359269], 'precision': 0.7063134160090192, 'sensitivity': 0.9642170065409773, 'no_links': 3548, 'F-score': 0.8153570847567918}\n",
      "nn bagging: {'no_false': 1100, 'confusion_matrix': [2506, 1007, 93, 359304], 'precision': 0.7133504127526331, 'sensitivity': 0.9642170065409773, 'no_links': 3513, 'F-score': 0.8200261780104713}\n",
      "\n",
      "lg per fold:\n",
      "Fold 0 {'no_false': 1684, 'confusion_matrix': [2511, 1596, 88, 358715], 'precision': 0.6113951789627465, 'sensitivity': 0.966140823393613, 'no_links': 4107, 'F-score': 0.7488815985684462}\n",
      "Fold 1 {'no_false': 1728, 'confusion_matrix': [2510, 1639, 89, 358672], 'precision': 0.6049650518197156, 'sensitivity': 0.9657560600230858, 'no_links': 4149, 'F-score': 0.7439241256668643}\n",
      "Fold 2 {'no_false': 1736, 'confusion_matrix': [2516, 1653, 83, 358658], 'precision': 0.6035020388582394, 'sensitivity': 0.9680646402462486, 'no_links': 4169, 'F-score': 0.7434988179669031}\n",
      "Fold 3 {'no_false': 1695, 'confusion_matrix': [2511, 1607, 88, 358704], 'precision': 0.6097620203982516, 'sensitivity': 0.966140823393613, 'no_links': 4118, 'F-score': 0.7476552032157213}\n",
      "Fold 4 {'no_false': 1695, 'confusion_matrix': [2511, 1607, 88, 358704], 'precision': 0.6097620203982516, 'sensitivity': 0.966140823393613, 'no_links': 4118, 'F-score': 0.7476552032157213}\n",
      "Fold 5 {'no_false': 1682, 'confusion_matrix': [2510, 1593, 89, 358718], 'precision': 0.6117475018279308, 'sensitivity': 0.9657560600230858, 'no_links': 4103, 'F-score': 0.7490301402566398}\n",
      "Fold 6 {'no_false': 1715, 'confusion_matrix': [2511, 1627, 88, 358684], 'precision': 0.6068148864185597, 'sensitivity': 0.966140823393613, 'no_links': 4138, 'F-score': 0.745435653851863}\n",
      "Fold 7 {'no_false': 1715, 'confusion_matrix': [2511, 1627, 88, 358684], 'precision': 0.6068148864185597, 'sensitivity': 0.966140823393613, 'no_links': 4138, 'F-score': 0.745435653851863}\n",
      "Fold 8 {'no_false': 1672, 'confusion_matrix': [2511, 1584, 88, 358727], 'precision': 0.6131868131868132, 'sensitivity': 0.966140823393613, 'no_links': 4095, 'F-score': 0.7502240812668061}\n",
      "Fold 9 {'no_false': 1709, 'confusion_matrix': [2511, 1621, 88, 358690], 'precision': 0.6076960309777347, 'sensitivity': 0.966140823393613, 'no_links': 4132, 'F-score': 0.7461001337097014}\n",
      "lg bagging: {'no_false': 1698, 'confusion_matrix': [2511, 1610, 88, 358701], 'precision': 0.6093181266682844, 'sensitivity': 0.966140823393613, 'no_links': 4121, 'F-score': 0.7473214285714287}\n",
      "\n",
      "STACKING PERFORMANCE:\n",
      "\n",
      "{'no_false': 999, 'confusion_matrix': [2505, 905, 94, 359406], 'precision': 0.7346041055718475, 'sensitivity': 0.9638322431704501, 'no_links': 3410, 'F-score': 0.8337493759360958}\n"
     ]
    }
   ],
   "source": [
    "## ENSEMBLE CLASSIFICATION AND EVALUATION\n",
    "\n",
    "print(\"BAGGING PERFORMANCE:\\n\")\n",
    "modeltypes = ['svm', 'nn', 'lg'] \n",
    "modeltypes_2 = ['rbf', 'relu', 'l2']\n",
    "modelparams = [0.001, 2000, 0.005]\n",
    "nFold = 10\n",
    "kf = KFold(n_splits=nFold)\n",
    "model_raw_score = [0]*3\n",
    "model_binary_score = [0]*3\n",
    "model_i = 0\n",
    "for model_i in range(3):\n",
    "    modeltype = modeltypes[model_i]\n",
    "    modeltype_2 = modeltypes_2[model_i]\n",
    "    modelparam = modelparams[model_i]\n",
    "    print(modeltype, \"per fold:\")\n",
    "    iFold = 0\n",
    "    result_fold = [0]*nFold\n",
    "    final_eval_fold = [0]*nFold\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        X_train_fold = X_train[train_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        md =  train_model(modeltype, modelparam, X_train_fold, y_train_fold, modeltype_2)\n",
    "        result_fold[iFold] = classify(md, X_test)\n",
    "        final_eval_fold[iFold] = evaluation(y_test, result_fold[iFold])\n",
    "        print(\"Fold\", str(iFold), final_eval_fold[iFold])\n",
    "        iFold = iFold + 1\n",
    "    bagging_raw_score = np.average(result_fold, axis=0)\n",
    "    bagging_binary_score  = np.copy(bagging_raw_score)\n",
    "    bagging_binary_score[bagging_binary_score > 0.5] = 1\n",
    "    bagging_binary_score[bagging_binary_score <= 0.5] = 0\n",
    "    bagging_eval = evaluation(y_test, bagging_binary_score)\n",
    "    print(modeltype, \"bagging:\", bagging_eval)\n",
    "    print('')\n",
    "    model_raw_score[model_i] = bagging_raw_score\n",
    "    model_binary_score[model_i] = bagging_binary_score\n",
    "    \n",
    "thres = .99\n",
    "print(\"STACKING PERFORMANCE:\\n\")\n",
    "stack_raw_score = np.average(model_raw_score, axis=0)\n",
    "stack_binary_score = np.copy(stack_raw_score)\n",
    "stack_binary_score[stack_binary_score > thres] = 1\n",
    "stack_binary_score[stack_binary_score <= thres] = 0\n",
    "stacking_eval = evaluation(y_test, stack_binary_score)\n",
    "print(stacking_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stack_raw_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mstack_raw_score\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'stack_raw_score' is not defined"
     ]
    }
   ],
   "source": [
    "print(stack_raw_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
